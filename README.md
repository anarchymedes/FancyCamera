# FancyCamera
I was suitably impressed by the [Create camera exensions with Core Media IO](https://developer.apple.com/wwdc22/10022) video from WWDC 2022 and suddenly, I had a thought: would it improve my chances of getting the job of my dreams if I entered an online job interview using something clearly of my own making? I realised that technically, I had no way of knowing what platfrom would be used during such an interview: MS Teams and Zoom being the likeliest candidates, Skype possible but unlikely (I've only had a couple of job interview on Skype), and FaceTime, WhatsApp, and the rest, _extremely_ unlikely. The idea of a 'creative' virtual camera seemed perfect for the purpose: it should theoretically be available on _all_ platforms that are good Mac OS citizens; however, not only the example the presenter shows in the aforementioned WWDC video is _not_ open-source and _not_ available for download, but it applies its effects (stylisation, distortion, and what not) to the _entire_ frame, including the person (me). That would not do for a job interview, would it? So, here was the challenge:
1. Figure out the way to use any hardware camera available on my mac as the source of frames that the virtual camera processes as desired and forwards to the CMIO extension as the virtual camera's output
2. As an integral part of the image processing mentioned above, separate the subject (the person) from the background, and apply any effects _only_ to the background
3. Add a feature the WWDC video does _not_ include: an animated background; use a loopable GIF for that, composing the separated foreground (the person) on top of it.

This is what this code does, in a nutshell. As a challenge, I decided to develop the app using SwiftUI and Combine.
## The Project
Obviously, to build the project you will need to replace the signing info: the team ID, etc., along with requesting the camera usage capability for the app. Be very, _very, very, very_ careful when modifying the extension-related stuff: the CMIO Mach Service name, Bundle ID, and so on: I had an _extremely_ hard time getting that part to work, none of the online recommendations working quite as expected; at the end, I was reduced to trial and error and blind luck, but it worked for me - eventually.üòâüòÅ
## The App
This app needs to be in the Applications folder to run. It launches as an ordinary SwiftUI Mac OS app; its window has the two buttons, Activate and Deactivate, which install and uninstall the extension, respectively. When installing the extension, note that all the interaction happens via the alerts: just find the relevant alert, click 'Open System Settings', allow the extension, and then close all the remaining alerts. For some reason, more often than not, you have to restart your computer before the app starts forwarding anything other that the default sliding white line to any camera client, such as FaceTime.
If the main window is closed by anything other than the big 'Power Off' button showing the associated icon, the app will remain running as a menu bar extra: clicking on the extra icon will re-open the main window as a 1024x768 popover.
While it's running, regardles of whether the window is open or not, it keeps the default camera on and can be used by any client: so, don't forget to shut it down by the big Power Off button when you're not using it; otherwise, it will likely have your battery and CPU for breakfast (you can also uninstall the extension prior to doing so, it you're planning to bin it - or you can simply delete the app from the Applications folder and let the system take care of the rest). I used it on my Mac Mini: never had a chance to use it on a laptop so, I myself don't know how it's going to behave when a laptop goes to sleep.
### Controls
At the lower section of the app's main window, there is a dropdown selection of the hardware camera to use; this list does not (at least, it _should not_) include the FancyCamera itself. Underneath, there is a dropdown selection of the effect. Select None to simply forward the frames from your hardware camera as-is. Selecting Animation will enable the Animation Background dropdown selection, where you can choose one of the four built-in GIFs for an animated background. When the Pre-process Background checkbox is selected, the app attempt to remove (black out) the foregoround (the person) from the background before the final compositing happens. Checking the HQ button will switch the segmentation (the person detection) mode from fast to balanced: this is slow and not very reliable for videos so, when it's checked, most of the changes are disabled.
**NOTE:** The app does _not_ remember the user settings between runs: it will only keep them until it's closed by the big button; I didn't expect it to be much used as-is so, I haven't implemented this part.
## Sources
- To implement the SswiftUI/Combine part, I heavily relied on the [Building a Camera App With SwiftUI and Combine](https://www.kodeco.com/26244793-building-a-camera-app-with-swiftui-and-combine) tutorial by Kodeko.
- The camera extension that forwards externally-supplied images to the output I based on the [cameraextension](https://github.com/ldenoue/cameraextension/tree/main) project, by [ldenoue](https://github.com/ldenoue), here on GitHub.
- I looked into the [How to Remove Backgrounds Using Core ML](https://img.ly/blog/how-to-remove-backgrounds-using-coreml) post for the help with the separation of the subject (the person) from the background: it mentions the VNGeneratePersonSegmentationRequest use, so I'm assuming the Vision API uses Core ML under the hood, because I didn't use it directly.
- Because it had become necessary to make the graphic processing asyncrhonous for the result to even approach an acceptable quality, I had to look for the ways to make Combine async-tolerant in the [Asynchronous programming with SwiftUI and Combine](https://peterfriese.dev/blog/2022/combine-vs-async).
- The GIFs came from [Tenor.com](https://tenor.com/en-GB/).
